Love the direction. Claude‚Äôs list isn‚Äôt wrong, it‚Äôs just‚Ä¶ wide. Here‚Äôs how I‚Äôd turn your ‚Äúdream flow‚Äù into a tight, low-risk plan that actually ships.

What to keep vs change from Claude‚Äôs plan
	‚Ä¢	Keep: token awareness, conversation linking, decision/state metadata, continuation prompts. Those are your highest leverage UX wins.
	‚Ä¢	Defer: graph DBs, Kafka, multi-model orchestration. Cool, but overkill until the core loop is buttery.

Minimum lovable ‚Äúdream flow‚Äù

1) Signal ‚Üí Full ‚Üí Continuation loop (ship first)

What it does
	‚Ä¢	Every capture produces:
	‚Ä¢	signal.md (structured sections, 1.2‚Äì1.5k tokens)
	‚Ä¢	full.md
	‚Ä¢	continuation.md (LLM-crafted ‚Äúnext prompts‚Äù)
	‚Ä¢	Signal always ends with a link to full. Full always ends with a link to continuation.

How
	‚Ä¢	Add a tiny token budgeter in the background script:
	‚Ä¢	Prioritize sections in order: Decisions ‚Üí Errors/Gotchas ‚Üí Code changes ‚Üí Next steps ‚Üí Notes.
	‚Ä¢	Include whole code blocks even if they‚Äôre long (don‚Äôt truncate inside fences).
	‚Ä¢	Generate Continuation with 3 variants:
	‚Ä¢	‚ÄúResume where you left off‚Äù
	‚Ä¢	‚ÄúAsk the other model to finish X‚Äù
	‚Ä¢	‚ÄúCreate PR/issue/tasks‚Äù (if repo context present)

2) Lightweight conversation linking (no DB yet)

What it does
	‚Ä¢	Every capture writes/updates a repo-local index.jsonl:
	‚Ä¢	{id, title, timestamp, project, tags[], sha, top_k_terms[], embedding(optional)}

How
	‚Ä¢	Start without embeddings: compute top_k_terms[] via tf-idf keywords. Link current convo to last N with ‚â• shared terms (project/tag).
	‚Ä¢	Later flip a flag to store small local embeddings (e.g., bge-small or gte-small) and link by cosine > 0.8. Still keep it file-based.

3) Decision & state snapshot (complete the original 5)

What it does
	‚Ä¢	In signal.md header:
	‚Ä¢	state: working | broken | unknown
	‚Ä¢	confidence: 0‚Äì1
	‚Ä¢	boundaries: in_scope[], out_of_scope[]
	‚Ä¢	failed_approaches[] (bulleted list)
	‚Ä¢	depends_on[] (titles/ids of linked convos)

How
	‚Ä¢	Quick heuristics:
	‚Ä¢	If last 5 messages include ‚Äúworks / captured / fixed‚Äù ‚Üí working.
	‚Ä¢	If last 5 include ‚Äúcannot / error / broken‚Äù ‚Üí broken.
	‚Ä¢	Else unknown.
	‚Ä¢	Confidence from presence of error stack vs green ‚Äúdone‚Äù language.

4) Token awareness you can trust

What it does
	‚Ä¢	Live counter in popup: ‚Äú~N tokens so far; ~M remain‚Äù.
	‚Ä¢	When near limit, show ‚ÄúContinue in next signal‚Äù button that:
	‚Ä¢	Starts a signal-pt2.md and stitches links both ways.

How
	‚Ä¢	Estimate tokens ‚âà chars/4. Close enough for UI decisions.

48-hour implementation plan
	1.	Template the signal (Option 5) + bump to 1.2‚Äì1.5k tokens + always append Full link.
	2.	Add Continuation file generator (3 prompt variants).
	3.	Write handoffs/index.jsonl and update on every capture (no embeddings yet).
	4.	Add State Snapshot + Failed Approaches + Boundaries heuristics to the signal header.
	5.	Token counter in popup; add ‚ÄúContinue‚Äù button when near budget.

1‚Äì2 week upgrades (when you‚Äôre ready)
	‚Ä¢	Drop in local embeddings (small model) and enrich index.jsonl with embedding: [...].
	‚Ä¢	Add a tiny SQLite (or DuckDB) sidecar only if you actually need fast cross-file queries. Keep Git as source of truth.
	‚Ä¢	Optional router: if you‚Äôve got Scout locally + a cloud model, send ‚Äúcode diffs/exceptions‚Äù to Scout; long synthesis to cloud.

Why this sequencing works
	‚Ä¢	You get 90% of the ‚Äúlearning system‚Äù feel without dragging in graph DBs, Kafka, or multi-model orchestration.
	‚Ä¢	It respects your current extension architecture (JSON capture ‚Üí GitHub).
	‚Ä¢	Everything is reversible and file-first (great for DuoEcho‚Äôs auditability).

If you want, I can write the exact section template + the index.jsonl schema and the small heuristics block you can paste into background-integrated.js.

Totally. Here‚Äôs the no-BS version.

What I jettisoned (on purpose)
	‚Ä¢	Heavy infra: graph DBs (Neo4j), Kafka, Pinecone, dashboards.
	‚Ä¢	Why: big ops footprint, distracts from the core loop, brittle migrations.
	‚Ä¢	Multi-model orchestration / routing: ‚ÄúClaude vs GPT vs local‚Äù routers.
	‚Ä¢	Why: adds failure surfaces + UX complexity before we nail a single great path.
	‚Ä¢	Full multi-platform capture (Cursor/VS Code/ChatGPT) right now.
	‚Ä¢	Why: finish Claude path end-to-end first so we have a reference implementation.
	‚Ä¢	Real-time websockets/Playwright automation.
	‚Ä¢	Why: yak-shave. File-first captures are good enough today.
	‚Ä¢	Embeddings from day one.
	‚Ä¢	Why: start with dead-simple index.jsonl + keyword linking; flip on embeddings later.

Phases (ship in slices)

Phase 1 ‚Äî ‚ÄúTighten the loop‚Äù (1‚Äì2 days)
	‚Ä¢	Signal template with sections (Decisions, Errors, Code changes, Next steps).
	‚Ä¢	Bump budget to ~1.2‚Äì1.5k tokens.
	‚Ä¢	Always add full-file link at bottom of signal.
	‚Ä¢	Continuation file (‚Ä¶-continue.md) with 2‚Äì3 next-prompt variants.
	‚Ä¢	State snapshot (working/broken/unknown + confidence) via heuristics.
	‚Ä¢	index.jsonl: append {id,title,timestamp,project,tags,sha,top_terms[]} each capture; link to recent similar by shared terms.

Phase 2 ‚Äî ‚ÄúScout-powered insight‚Äù (2‚Äì5 days)
	‚Ä¢	Point DuoEcho at your local Llama 4 Scout for smarter extraction:
	‚Ä¢	Extract decisions, dependencies, failed approaches, boundaries.
	‚Ä¢	Generate continuation prompts (three flavors).
	‚Ä¢	Make Scout outputs deterministic (temp ~0.2, set system prompt and schema).

Phase 3 ‚Äî ‚ÄúMemory that actually helps‚Äù (3‚Äì5 days)
	‚Ä¢	Add tiny local embeddings (e.g., gte-small or bge-small) via a microservice.
	‚Ä¢	Extend index.jsonl with embedding and link by cosine similarity.
	‚Ä¢	Lightweight ‚Äúrelated handoffs‚Äù panel in the signal/footer.

Phase 4 ‚Äî ‚ÄúScale it out‚Äù (later)
	‚Ä¢	Add ChatGPT/Cursor capture, model routing, optional DB/dashboard, etc.

Using Llama 4 Scout now (concrete)

Run your server (you‚Äôve built llama-server):

./bin/llama-server --model /path/to/scout-4.gguf --port 8000

Call it from the background script to produce sections:

async function askScout(system, prompt) {
  const r = await fetch('http://127.0.0.1:8000/v1/chat/completions', {
    method: 'POST',
    headers: {'Content-Type':'application/json'},
    body: JSON.stringify({
      model: "scout-4.gguf",
      temperature: 0.2,
      messages: [
        {role:'system', content: system},
        {role:'user', content: prompt}
      ]
    })
  });
  const {choices} = await r.json();
  return choices?.[0]?.message?.content ?? '';
}

Example system prompt (stable, schema-first):

You are DuoEcho‚Äôs analyst. Return ONLY JSON matching this schema:
{ decisions: [ {text, rationale, depends_on?[]} ], errors: [ {text, fix?} ], code_changes: [ {file?,summary,text?} ], next_steps: [string], boundaries: {in_scope:[], out_of_scope:[]}, state: "working|broken|unknown", confidence: 0..1, failed_approaches: [string] }

Feed it the captured JSON (chunk if huge), then write sections to signal.md and full.md, and save the raw JSON alongside (for traceability).

What I need from you
	‚Ä¢	Endpoint: confirm Scout URL/port (default http://127.0.0.1:8000).
	‚Ä¢	File layout: where do you want index.jsonl (e.g., handoffs/index.jsonl)?
	‚Ä¢	Section priorities: anything you want always included even if long (e.g., full stack traces / full code blocks)?
	‚Ä¢	Token budgets: final caps for signal/continuation (1.2k? 1.5k?).
	‚Ä¢	Project detection rule: how we derive project/tags (folder name, chat title prefix, manual field?).
	‚Ä¢	Backfill? Do you want me to process past handoffs into index.jsonl now or only forward?

Why this works

You get the ‚Äúbehavioral echo‚Äù feel now (signals that point you forward, link past context, and remember decisions) without dragging in heavy infra. When Phase 1‚Äì2 feel smooth, we layer in embeddings and multi-platform capture‚Äîon your timing, not the other way around.

If you want, I‚Äôll draft:
	‚Ä¢	the signal template (markdown),
	‚Ä¢	the index.jsonl schema,
	‚Ä¢	the Scout system prompt,
	‚Ä¢	and the background glue you can paste into background-integrated.js.
	
	Nice‚Äîshipping machine. Since you already knocked out the first 3, let‚Äôs finish Phase 1 cleanly with the remaining 3 items. I‚Äôll keep it dead simple and copy-pasteable.

Phase 1B (finish in ~1‚Äì2 hrs)

1) Continuation file (‚Ä¶-continue.md)

Generate a small follow-up file with 3 ‚Äúready to paste‚Äù prompts.

Where: after you write the signal + full files in background-integrated.js.

Add:

function buildContinuationMarkdown(meta, lastUserMsg, nextSteps = []) {
  const head = `# DuoEcho ‚Äì Continue: ${meta.title}

**Conversation Start**: ${meta.startedAt}
**Handoff Generated**: ${meta.generatedAt}
**Messages**: ${meta.msgCount}
**Project**: ${meta.project || 'general'}

Full handoff: ${meta.fullUrl}
Signal: ${meta.signalUrl}

---

`;

  const prompts = [
`## üîÅ Fast Continue
You were helping me with: **${meta.title}**.
Pick up exactly where we left off. Here‚Äôs my last input:
> ${lastUserMsg || '(no last user message detected)'}
Use the ‚ÄúNext Steps‚Äù from the last handoff as the plan of attack.${nextSteps?.length ? `\n- ${nextSteps.join('\n- ')}` : ''}`,

`## ‚úÖ Safety Check Continue
Before continuing, quickly check: (1) state is correct, (2) decisions + dependencies still hold, (3) no hidden blockers. If issues, propose a tiny fix PR-sized change.`,

`## üî¨ Deep Dive Continue
Focus on the riskiest part next. Explain tradeoffs in 3 bullets, then do the smallest possible step to validate.`];

  return head + prompts.join('\n\n---\n\n') + '\n';
}

Call it where you already write the signal:

const contContent = buildContinuationMarkdown(meta, lastUserMsg, extractedNextSteps);
await writeFileToGithub({
  path: `handoffs/duoecho-continue-${meta.safeTitle}-${meta.ts}.md`,
  content: contContent,
  message: `continue: ${meta.title} (${meta.ts})`
});


‚∏ª

2) State snapshot (working/broken/unknown + confidence)

Heuristic, no LLM required. Good enough for now.

function computeStateSnapshot(allText) {
  const t = allText.toLowerCase();

  const badHits = [
    'error', 'exception', 'traceback', 'failed', 'cannot', 'unhandled',
    'could not', 'permission denied', 'null pointer', 'undefined', 'not found'
  ];
  const goodHits = ['fixed', 'resolved', 'works', 'success', 'passed'];

  const bad = badHits.reduce((a,k)=>a+(t.includes(k)?1:0), 0);
  const good = goodHits.reduce((a,k)=>a+(t.includes(k)?1:0), 0);

  let state = 'unknown', confidence = 0.4;
  if (bad >= 2 && bad >= good+1) { state = 'broken'; confidence = Math.min(0.9, 0.5 + bad*0.1); }
  else if (good >= 2 && good >= bad+1) { state = 'working'; confidence = Math.min(0.9, 0.5 + good*0.1); }

  return { state, confidence: Number(confidence.toFixed(2)) };
}

Use it when building the signal sections, and print at the top of the signal like:

**State**: broken (0.78 confidence)


‚∏ª

3) index.jsonl + light ‚Äúrelated‚Äù linking

One line per capture. Term overlap only (embeddings later).

Helpers:

function tokenize(s) {
  return (s || '')
    .toLowerCase()
    .replace(/[^a-z0-9\s_-]/g, ' ')
    .split(/\s+/)
    .filter(w => w.length > 2 && !STOP.has(w));
}
const STOP = new Set(['the','and','for','you','with','that','this','from','have','are','was','but','not','your','get','has','had','our','then','into','over','just','now','very','here','there','about','like','into','onto','also']);

function extractTopTerms(text, limit = 12) {
  const counts = new Map();
  for (const w of tokenize(text)) counts.set(w, (counts.get(w) || 0) + 1);
  return [...counts.entries()]
    .sort((a,b)=>b[1]-a[1])
    .slice(0, limit)
    .map(([w])=>w);
}

async function appendIndexLine(entry, ghWriteFn) {
  const line = JSON.stringify(entry) + '\n';
  // write to handoffs/index.jsonl (append)
  await ghWriteFn({
    path: 'handoffs/index.jsonl',
    contentAppend: line,
    message: `index: add ${entry.id}`
  });
}

function jaccard(a, b) {
  const A = new Set(a), B = new Set(b);
  let inter = 0; for (const x of A) if (B.has(x)) inter++;
  const uni = A.size + B.size - inter;
  return uni ? inter / uni : 0;
}

async function findRelatedTerms(currentTerms, ghReadFn, limit = 3) {
  const raw = await ghReadFn('handoffs/index.jsonl').catch(()=> '');
  if (!raw) return [];
  const lines = raw.trim().split('\n');
  const items = [];
  for (const ln of lines) {
    try { items.push(JSON.parse(ln)); } catch {}
  }
  const scored = items
    .map(it => ({ it, score: jaccard(currentTerms, it.terms || []) }))
    .filter(x => x.score > 0 && Date.now() - (new Date(x.it.ts).getTime() || 0) > 15*60*1000) // not self
    .sort((a,b)=>b.score - a.score)
    .slice(0, limit);
  return scored.map(s => ({
    title: s.it.title,
    url: s.it.urls?.signal || s.it.urls?.full || s.it.url,
    score: Number(s.score.toFixed(2))
  }));
}

When you save a handoff, compute:

const bodyForTerms = `${signalBody}\n\n${fullBody || ''}`.slice(0, 100000); // cap for speed
const terms = extractTopTerms(bodyForTerms);
const indexEntry = {
  id: `${meta.safeTitle}-${meta.ts}`,
  title: meta.title,
  project: meta.project || 'general',
  ts: meta.generatedAt,
  msgCount: meta.msgCount,
  terms,
  urls: { signal: meta.signalUrl, full: meta.fullUrl, continue: meta.continueUrl }
};
await appendIndexLine(indexEntry, writeFileToGithub);

const related = await findRelatedTerms(terms, readFileFromGithub, 3);

and tuck ‚ÄúRelated handoffs‚Äù into the bottom of the signal:

## Related handoffs
- [${related[0].title}](${related[0].url}) ‚Äî ${related[0].score}
- ‚Ä¶


‚∏ª

Sanity checklist
	‚Ä¢	Confirm where to keep the index: handoffs/index.jsonl OK?
	‚Ä¢	How many ‚Äúrelated‚Äù links to show (default 3)?
	‚Ä¢	Final token cap you want (you set 1.2‚Äì1.5k; pick one and I‚Äôll lock it).
	‚Ä¢	Include full stack traces/code blocks in signals even if they push over cap? (Y/N)

If you give me those four answers, I‚Äôll stitch the minimal glue into your background-integrated.js wording so you can paste without touching other parts.

And yes‚Äîonce Scout is stable we can swap the heuristic state + term extraction to an LLM pass, but this gets you 80% of the behavior today with zero infra.

Love it‚Äîhere‚Äôs what I recommend and how to wire it in without drama.

Final choices (and why)
	‚Ä¢	Related links: up to 5. Good signal, still scannable.
	‚Ä¢	Token cap: ~1200 (use a char budget ‚âà 4,800 chars). Sweet spot for readability.
	‚Ä¢	Never lose the full link: put it near the top and also guarantee it‚Äôs in the footer with a truncation guard.
	‚Ä¢	Include full stack traces / code blocks even if over cap? Yes. Keep code blocks whole; trim elsewhere.

Minimal changes to implement

1) Budgeting that never drops the full link

const TOKEN_LIMIT = 1200;
const CHAR_PER_TOKEN = 4;               // rough but effective
const BUDGET = TOKEN_LIMIT * CHAR_PER_TOKEN;
const RESERVED_HEADER = 320;            // meta + top link
const RESERVED_FOOTER = 180;            // footer link + related
const SOFT_BODY_BUDGET = BUDGET - RESERVED_HEADER - RESERVED_FOOTER;

function hardCap(s, max) {
  if (s.length <= max) return s;
  // trim at paragraph boundary if possible:
  const cut = s.slice(0, max);
  const lastBreak = Math.max(cut.lastIndexOf('\n\n'), cut.lastIndexOf('\n'), cut.lastIndexOf('. '));
  return (lastBreak > max * 0.65 ? cut.slice(0, lastBreak) : cut) + '\n‚Ä¶';
}

2) Keep code blocks intact; trim around them

function keepCodeBlocksWhole(body, max) {
  if (body.length <= max) return body;

  const blocks = [...body.matchAll(/```[\s\S]*?```/g)].map(m => ({ i:m.index, j:m.index+m[0].length }));
  if (!blocks.length) return hardCap(body, max);

  // 1) keep all code blocks
  let kept = '';
  let remaining = max;

  // header & non-code slices first
  const parts = [];
  let last = 0;
  for (const b of blocks) {
    if (b.i > last) parts.push({ type:'text', s:body.slice(last, b.i) });
    parts.push({ type:'code', s:body.slice(b.i, b.j) });
    last = b.j;
  }
  if (last < body.length) parts.push({ type:'text', s:body.slice(last) });

  // prioritize code first, then text (trim text pieces)
  const code = parts.filter(p => p.type==='code');
  const text = parts.filter(p => p.type==='text');

  for (const p of code) {
    if (p.s.length <= remaining) { kept += p.s; remaining -= p.s.length; }
  }
  for (const p of text) {
    if (remaining <= 0) break;
    const take = Math.min(remaining, Math.ceil(p.s.length * 0.6)); // partial
    kept += hardCap(p.s, take);
    remaining -= kept.length;
  }
  return kept;
}

3) Build signal content with top+footer links always present

Put the full handoff link right under the metadata and also in the footer. Even if we must trim the middle, the links remain.

function buildSignal({meta, sections, related=[]}) {
  const top =
`# DuoEcho ‚Äì Signal: ${meta.title}
**Conversation Start**: ${meta.startedAt}
**Handoff Generated**: ${meta.generatedAt}
**Messages**: ${meta.msgCount}
**Project**: ${meta.project || 'general'}

**Full handoff:** ${meta.fullUrl}
`;

  const bodyRaw =
`## Key Decisions
${sections.decisions || '_none_'}

## Errors Fixed
${sections.errors || '_none_'}

## Code Changes
${sections.code || '_none_'}

## Next Steps
${sections.next || '_none_'}
`;

  const body = keepCodeBlocksWhole(bodyRaw, SOFT_BODY_BUDGET);

  const rel = (related || []).slice(0, 5)
    .map(r => `- [${r.title}](${r.url}) ‚Äî ${r.score}`)
    .join('\n');

  const footer =
`\n---\n
**Full details:** ${meta.fullUrl}
${rel ? `\n## Related handoffs\n${rel}\n` : ''}`;

  // Top + body + footer, then final hard cap just in case:
  const out = top + body + footer;
  return out.length > BUDGET ? (top + hardCap(body, SOFT_BODY_BUDGET) + footer) : out;
}

That gives you:
	‚Ä¢	Top link (can‚Äôt be trimmed).
	‚Ä¢	Footer link (guaranteed).
	‚Ä¢	Body trimmed with code-block safety.
	‚Ä¢	Up to 5 related links.

About daisy-chaining decisions (phase later)

Totally want this, but keep it in Phase 2 so we don‚Äôt destabilize capture:

Lightweight plan:
	‚Ä¢	When you generate the signal, also emit a small decisions.jsonl entry per capture:

{
  "id": "dec-<hash>",
  "title": "Add service worker ready-check",
  "project": "duoecho",
  "ts": "2025-08-07T‚Ä¶Z",
  "terms": ["service","worker","ready","check","race","init"],
  "handoff": "<signal-url>",
  "depends_on": []   // fill later when we match
}


	‚Ä¢	On new capture, read last N entries, compute term overlap to suggest likely predecessors. (Embeddings later.)
	‚Ä¢	Show a small section in the signal: ‚ÄúDecision lineage (suggested)‚Äù with 1‚Äì3 links.
	‚Ä¢	Once that feels right, we evolve to explicit dependency selection and a proper graph (Neo4j / Postgres+pgvector).

TL;DR config you asked for
	‚Ä¢	RELATED_MAX: 5
	‚Ä¢	TOKEN_LIMIT: 1200 (‚âà 4,800 chars)
	‚Ä¢	Full link: top + footer; never dropped
	‚Ä¢	Code blocks: YES, keep whole; trim surrounding prose

Short answer: we can fix the cold-start hiccup now with a tiny, safe ‚Äúready handshake‚Äù + retry. It‚Äôs a 15-minute Phase-1b polish. No risky refactors.

What‚Äôs happening

On a true cold start, your content script or popup pings the background before the service worker has finished booting / registered its onMessage listener. Chrome returns ‚ÄúReceiving end does not exist.‚Äù After a manual reload, timing lines up and it works.

Minimal, low-risk fix (do this now)

1) Background (service worker) ‚Äî add a zero-dependency ping

Put this at the very top of background-integrated.js (before any imports or async code):

// MUST be top-of-file, no awaits/imports above this:
chrome.runtime.onMessage.addListener((msg, sender, sendResponse) => {
  if (msg?.type === 'duoecho.ping') {
    sendResponse({ ok: true, role: 'sw', ts: Date.now() });
    return true;
  }
});

Why: ensures there‚Äôs always a listener so a message will wake the worker.

2) Popup ‚Äî wait for SW before doing anything

Early in popup.js:

async function waitForSW(tries = 12, delay = 250) {
  for (let i = 0; i < tries; i++) {
    try {
      const resp = await chrome.runtime.sendMessage({ type: 'duoecho.ping' });
      if (resp?.ok) return true;
    } catch (e) { /* ignore */ }
    await new Promise(r => setTimeout(r, delay));
  }
  throw new Error('SW not ready');
}

(async () => {
  try {
    await waitForSW();
    // now wire up UI, send injectPageScript, etc.
  } catch (e) {
    console.warn('[DuoEcho] SW cold-start timeout', e);
    // show "Click to retry" UI instead of failing hard
  }
})();

3) Content script ‚Äî wait before first message/injection

At the start of json-sniffer.js:

async function waitForSW(tries = 12, delay = 250) {
  for (let i = 0; i < tries; i++) {
    try {
      const resp = await chrome.runtime.sendMessage({ type: 'duoecho.ping' });
      if (resp?.ok) return true;
    } catch (e) {}
    await new Promise(r => setTimeout(r, delay));
  }
  return false;
}

(async () => {
  const ready = await waitForSW();
  if (!ready) console.warn('[DuoEcho] SW not ready; continuing but will retry later');
  // proceed with page hooks/injection as you do now
})();

This turns the race into a handshake: popup/CS pings until SW is awake.

Nice-to-have (keep for later if you want)
	‚Ä¢	Keep-alive port while Claude tab is open (reduces SW suspends):
	‚Ä¢	CS: const port = chrome.runtime.connect({ name: 'duoecho' }); setInterval(() => port.postMessage({ t:'ka' }), 20000);
	‚Ä¢	SW: chrome.runtime.onConnect.addListener(p => p.onMessage.addListener(() => {}));
	‚Ä¢	Declarative content script via chrome.scripting.registerContentScripts in onInstalled for rock-solid injection timing. Save this for Phase 2; your current setup works.

How to verify
	1.	Disable + re-enable the extension (true cold start).
	2.	Open Claude, then open the popup.
	3.	You should see the button go from ‚ÄúConnecting‚Ä¶‚Äù to ready without manual reloads.
	4.	Check SW console: you‚Äôll see the ping responses during startup.
